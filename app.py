# -*- coding: utf-8 -*-
# ================== 1) –ò–º–ø–æ—Ä—Ç—ã ==================
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã.
import os
import pathlib
import gc
import streamlit as st # –ò—Å–ø–æ–ª—å–∑—É–µ–º Streamlit –≤–º–µ—Å—Ç–æ Gradio
from typing import List, Tuple
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from llama_cpp import Llama

# ================== 2) –ü—É—Ç–∏ ==================
# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –≤–∞—à–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
DOCS_DIR = "docs"

# –ü—É—Ç—å, –≥–¥–µ –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è —á–∏—Å–ª–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
INDEX_DIR = "FAISS_Index"
os.makedirs(INDEX_DIR, exist_ok=True)

# ================== 3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ ==================
# –≠—Ç–æ—Ç –±–ª–æ–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–∑–¥–∞–Ω –ª–∏ —É–∂–µ –∏–Ω–¥–µ–∫—Å.
# –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞–µ—Ç –µ–≥–æ, –µ—Å–ª–∏ –µ—Å—Ç—å ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ—Ç.
if not os.path.exists(os.path.join(INDEX_DIR, "index.faiss")):
    st.info("–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –ø–∞–ø–∫–∏ 'docs'
    loader = PyPDFDirectoryLoader(DOCS_DIR, recursive=True)
    all_docs = loader.load()

    if not all_docs:
        st.error("PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–ª–æ–∂–∏—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É 'docs'.")
        st.stop() # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150)
    chunks = splitter.split_documents(all_docs)
    st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(all_docs)} | –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(chunks)}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤).
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å FAISS –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    vs = FAISS.from_documents(chunks, emb)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –µ–≥–æ –∑–∞–Ω–æ–≤–æ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ.
    vs.save_local(INDEX_DIR)
    del chunks
    gc.collect()
    
st.success("‚úÖ –ò–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤!")

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.
emb = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
vs = FAISS.load_local(INDEX_DIR, emb, allow_dangerous_deserialization=True)
retriever = vs.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# ================== 4) –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é LLM (—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å) ==================
# –≠—Ç–æ—Ç –±–ª–æ–∫ –∑–∞–≥—Ä—É–∂–∞–µ—Ç "–º–æ–∑–≥" –≤–∞—à–µ–≥–æ —á–∞—Ç-–±–æ—Ç–∞.
@st.cache_resource # –ö—ç—à–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–ª–∞—Å—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏
def get_llm():
    return Llama.from_pretrained(
        repo_id="Qwen/Qwen2.5-1.5B-Instruct-GGUF",
        filename="*q4_k_m.gguf",
        n_ctx=4096,
        n_threads=8,
        n_gpu_layers=35, # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ 0, –µ—Å–ª–∏ —É –≤–∞—Å –Ω–µ—Ç GPU
        verbose=False,
    )

llm = get_llm()

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ò–ò-–º–æ–¥–µ–ª–∏, –∫–∞–∫–æ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –µ–µ "–ª–∏—á–Ω–æ—Å—Ç—å".
SYSTEM_PROMPT = (
    "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –∏–Ω–∂–µ–Ω–µ—Ä–∞-—Å—Ç—Ä–æ–∏—Ç–µ–ª—è –≤ –†–µ—Å–ø—É–±–ª–∏–∫–µ –ë–µ–ª–∞—Ä—É—Å—å. "
    "–û—Ç–≤–µ—á–∞–π –∫–æ—Ä–æ—Ç–∫–æ (–º–∞–∫—Å. 14-20 —Å—Ç—Ä–æ–∫), —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º. "
    "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ–≥–æ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç ‚Äî –ø—Ä—è–º–æ —Å–∫–∞–∂–∏, —á—Ç–æ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —ç—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. "
    "–í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å —Å—Ç—Ä–æ–∫—É '–ò—Å—Ç–æ—á–Ω–∏–∫–∏:' —Å–æ —Å–ø–∏—Å–∫–æ–º —Ñ–∞–π–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç–æ–≤."
)

def build_context(docs, limit_chars=3500) -> Tuple[str, List[str]]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤."""
    ctx, cites, total = [], [], 0
    for d in docs:
        src = os.path.basename(d.metadata.get("source", ""))
        page = (d.metadata.get("page") or 0) + 1
        header = f"[{src}, —Å—Ç—Ä. {page}]"
        text = (d.page_content or "").strip().replace("\u0000", " ").replace("\t", " ")
        if not text:
            continue
        snippet = text[:1200]
        ctx.append(f"{header}\n{snippet}")
        cites.append(header)
        total += len(snippet)
        if total >= limit_chars:
            break
    return "\n\n".join(ctx), cites

def rag_answer(question: str) -> str:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    # 1) –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫—É—Å–∫–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.
    docs = retriever.get_relevant_documents(question)
    if not docs:
        return "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –Ω—É–∂–Ω—ã–µ PDF –∑–∞–≥—Ä—É–∂–µ–Ω—ã."
    
    context, cites = build_context(docs)

    # 2) –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥ –¥–ª—è –ò–ò-–º–æ–¥–µ–ª–∏.
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"–í–æ–ø—Ä–æ—Å: {question}\n\n–§—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{context}"}
    ]

    # 3) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç.
    output = llm.create_chat_completion(
        messages=messages,
        temperature=0.2,
        top_p=0.9,
        max_tokens=320
    )
    text = output["choices"][0]["message"]["content"].strip()

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–±—ã–ª–∞ –∏—Ö —É–∫–∞–∑–∞—Ç—å.
    if "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:" not in text:
        text += "\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(cites)

    return text

# ================== 5) –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á–∞—Ç–∞ (Streamlit) ==================
# –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", page_icon="üèóÔ∏è")
st.title("üèóÔ∏è –°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç")
st.markdown("–ü–æ–∏—Å–∫ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∞—à–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
if "messages" not in st.session_state:
    st.session_state.messages = []

# –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
if prompt := st.chat_input("–ó–∞–¥–∞–π —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å"):
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    with st.chat_message("assistant"):
        with st.spinner("–î—É–º–∞—é..."):
            response = rag_answer(prompt)
        st.markdown(response)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
    st.session_state.messages.append({"role": "assistant", "content": response})

